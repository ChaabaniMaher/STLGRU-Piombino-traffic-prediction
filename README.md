# ğŸš¦ STLGRU: Traffic Flow Prediction for Piombino

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red)
![License](https://img.shields.io/badge/License-MIT-green)
![GitHub last commit](https://img.shields.io/github/last-commit/ChaabaniMaher/STLGRU-Piombino)

---

## ğŸ“‹ Project Overview

This project implements **STLGRU (Spatio-Temporal Lightweight Graph GRU)** for traffic flow prediction using real-world data from **Piombino, Italy**. The model predicts traffic flow **45 minutes ahead** using **3 hours of historical data** from 4 road sensors (STM1â€“STM4). The implementation follows the methodology described in the STLGRU paper by Bhaumik et al. (PAKDD 2024).

### Key Features

- Complete data preparation pipeline (cleaning, pivoting, normalization, graph construction)
- Correlation-based adjacency matrix capturing spatial dependencies
- Temporal sequence creation (12 steps â†’ 3 steps)
- Chronological train/validation/test split (70/15/15) with no data leakage
- Lightweight STLGRU model with only **20k parameters**
- Achieved **RÂ² = 0.91** for 15-minute predictions and **0.89** for 45-minute predictions

---

## ğŸ“Š Dataset Description

The dataset consists of traffic flow measurements from four sensors located in Piombino, Italy, recorded at 15-minute intervals from May 2024 to February 2025.

| Attribute              | Value                               |
|------------------------|-------------------------------------|
| Time period            | May 2024 â€“ February 2025            |
| Time steps             | 52,700                              |
| Sensors                | 4 (STM1, STM2, STM3, STM4)          |
| Raw records            | 368,956                             |
| Training samples       | 36,880 (70%)                        |
| Validation samples     | 7,902 (15%)                         |
| Test samples           | 7,904 (15%)                         |
| History length         | 12 steps (3 hours)                  |
| Prediction horizon     | 3 steps (45 minutes)                |

---

## ğŸ—ï¸ Methodology

### Data Preparation Pipeline

1. Load raw data (Parquet file) and select relevant columns (`Date_out`, `Sensor`, `Flux`).
2. Clean data by converting `Date_out` to datetime and filling missing `Flux` values with 0.
3. Pivot to matrix format (rows = timestamps, columns = sensors) with shape `(52700, 4)`.
4. Normalize using `StandardScaler` fitted only on training data.
5. Build graph by computing the correlation matrix from training data, applying a threshold of 0.5 to create the adjacency matrix, and adding self-loops. The resulting 4Ã—4 matrix has 75 percent density.
6. Create sequences using a sliding window: 12 past steps â†’ 3 future steps. Final shapes: `X (52686, 12, 4)` and `Y (52686, 3, 4)`.
7. Apply temporal split: 70 percent training, 15 percent validation, and 15 percent test while preserving chronological order.

### Model Architecture

The STLGRU model consists of:

- Input: batch Ã— 12 Ã— 4
- Two-layer GRU with hidden size 64 and dropout 0.1
- Linear layer: 64 â†’ 12 (3 horizons Ã— 4 sensors)
- Output: batch Ã— 3 Ã— 4
- Total parameters: 20,032

**Loss function:** MSE  
**Optimizer:** Adam (learning rate 0.001)  
**Training epochs:** 50  
**Batch size:** 32  

---

## ğŸ“ˆ Results

### Test Performance by Horizon

| Horizon   | MAE (vehicles) | RMSE (vehicles) | MAPE*  | RÂ²    |
|-----------|----------------|-----------------|--------|-------|
| 15 min    | 41.59          | 64.92           | 43.14% | 0.9266 |
| 30 min    | 46.91          | 74.71           | 52.55% | 0.9028 |
| 45 min    | 50.22          | 79.75           | 49.12% | 0.8892 |
| Average   | 46.24          | 73.13           | 48.27% | 0.9062 |

\* MAPE computed only on non-zero traffic values.

- Best validation loss: 0.1864 (epoch 35)
- Training time: 526.54 seconds (approximately 8.8 minutes)

### Visualizations

| Training curves | Predictions vs Actual (STM1, 15 min) |
|-----------------|--------------------------------------|
| ![Training curves](training_curves.png) | ![Predictions](predictions_vs_actual.png) |

---

## ğŸ”§ Installation

### Prerequisites

- Python 3.8+
- pip
- Git

### Clone the repository

```bash
git clone https://github.com/ChaabaniMaher/STLGRU-Piombino.git
cd STLGRU-Piombino

ğŸ“‹Set up virtual environment
python3 -m venv venv
source venv/bin/activate      # Linux/Mac
# .\venv\Scripts\activate     # Windows

ğŸ“‹Install dependencies





ğŸ“‹Required packages
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
torch>=2.0.0
pyarrow>=10.0.0
fastparquet>=2023.0.0
matplotlib>=3.5.0
seaborn>=0.12.0
tqdm>=4.65.0



ğŸ“– Usage
1. Data Preparation
python data_preparation_piombino.py

2. Train the Model
python train_simple.py

The script will:

Load the preâ€‘processed data

Train the STLGRU model for 50 epochs

Save the best model as best_model_simple.pth

Save training history as training_results.pkl

3. Evaluate and Visualize
python evaluate_results.py
 
4.Use the Trained Model for Inference
import torch
import numpy as np
import pickle
from train_simple import SimpleSTLGRU

# Load model
device = torch.device('cpu')
checkpoint = torch.load('best_model_simple.pth', map_location=device)
model = SimpleSTLGRU(**checkpoint['config'])
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Load scaler for inverse transform
with open('scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)

# Example: predict on new data (shape: (1, 12, 4))
new_data = ...  # your 12 time steps for 4 sensors
with torch.no_grad():
    pred_scaled = model(torch.FloatTensor(new_data).unsqueeze(0))
    pred = scaler.inverse_transform(pred_scaled.numpy().reshape(-1,4)).reshape(1,3,4)
print(pred)

ğŸ“ Project Structure

STLGRU-Piombino/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ data_preparation_piombino.py
â”œâ”€â”€ train_simple.py
â”œâ”€â”€ evaluate_results.py
â”œâ”€â”€ best_model_simple.pth         # Trained model
â”œâ”€â”€ training_results.pkl           # Training history
â”œâ”€â”€ training_curves.png            # Loss/MAE curves
â”œâ”€â”€ predictions_vs_actual.png       # Predictions plot
â”œâ”€â”€ scaler.pkl                      # StandardScaler object
â”œâ”€â”€ adj_matrix.npy                   # Adjacency matrix
â”œâ”€â”€ X_train.npy, Y_train.npy         # Processed data (optional)
â”œâ”€â”€ X_val.npy, Y_val.npy
â”œâ”€â”€ X_test.npy, Y_test.npy
â””â”€â”€ venv/                            # Virtual environment (ignored)

ğŸ”‘ Key Findings
Temporal split is crucial â€“ chronological order prevents data leakage; random split would overestimate performance.

Graph reflects reality â€“ high correlation (75% density) between sensors in a small city.

12â€‘step history is optimal â€“ longer histories did not improve performance.

Excellent predictive capability â€“ average RÂ² = 0.91, MAE = 46 vehicles.

Lightweight design â€“ only 20k parameters, suitable for edge deployment.

ğŸš€ Future Improvements
Incorporate weather data and holiday calendar

Add attention mechanisms

Test on other cities

Deploy as a realâ€‘time API with a web dashboard

ğŸ‘¤ Author
Maher Chaabani
ğŸ“§ chaabanimaher9@gmail.com
ğŸ”— GitHub
ğŸ“ Tunisia
# STLGRU-Piombino-traffic-prediction
